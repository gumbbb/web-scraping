{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385d939d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24-06-23'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "from datetime import date\n",
    "today= date.today();\n",
    "\n",
    "today = today.strftime(\"%d-%m-%y\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b424f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "sport=[]\n",
    "tag = 'Thể thao'\n",
    "# URL của trang web gốc\n",
    "url = 'https://vnexpress.net/the-thao'\n",
    "\n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "for news_title in news_titles:\n",
    "    title = news_title.text.strip()\n",
    "    link_element = news_title.find('a')\n",
    "    news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "    response = requests.get(news_link)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "    news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "    content_elements = news_soup.find_all('p', class_='Normal')\n",
    "    content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "    sport.append([tag,title,content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9856229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sport_df=pd.DataFrame(sport,columns=['Tag','Title','Content'])\n",
    "\n",
    "sport_df.to_csv('.\\Data\\sport.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f7a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_news=[]\n",
    "tag = 'Thời sự'\n",
    "# URL của trang web gốc\n",
    "url = 'https://vnexpress.net/thoi-su'\n",
    "\n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "for news_title in news_titles:\n",
    "    title = news_title.text.strip()\n",
    "    link_element = news_title.find('a')\n",
    "    news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "    response = requests.get(news_link)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "    news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "    content_elements = news_soup.find_all('p', class_='Normal')\n",
    "    content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "    daily_news.append([tag,title,content])\n",
    "daily_news_df=pd.DataFrame(sport,columns=['Tag','Title','Content'])\n",
    "\n",
    "daily_news_df.to_csv('.\\Data\\daily news.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ec9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "worlds=[]\n",
    "tag = 'Thế giới'\n",
    "# URL của trang web gốc\n",
    "url = 'https://vnexpress.net/the-gioi'\n",
    "\n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "for news_title in news_titles:\n",
    "    title = news_title.text.strip()\n",
    "    link_element = news_title.find('a')\n",
    "    news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "    response = requests.get(news_link)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "    news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "    content_elements = news_soup.find_all('p', class_='Normal')\n",
    "    content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "    worlds.append([tag,title,content])\n",
    "worlds_df=pd.DataFrame(sport,columns=['Tag','Title','Content'])\n",
    "\n",
    "worlds_df.to_csv('.\\Data\\worlds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38570e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
