{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a293b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25-06-23'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "from datetime import date\n",
    "today= date.today();\n",
    "\n",
    "today = today.strftime(\"%d-%m-%y\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddae6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "sport=[]\n",
    "tag = 'Thể thao'\n",
    "# URL của trang web gốc\n",
    "url = 'https://vnexpress.net/bong-da'\n",
    "\n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "for news_title in news_titles:\n",
    "    title = news_title.text.strip()\n",
    "    link_element = news_title.find('a')\n",
    "    news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "    response = requests.get(news_link)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "    news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "    content_elements = news_soup.find_all('p', class_='Normal')\n",
    "    content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "    sport.append([tag,title + content])\n",
    "sport_df=pd.DataFrame(sport,columns=['Tag','Content'])\n",
    "\n",
    "sport_df.to_csv('.\\data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab14d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sport=[]\n",
    "tag = 'Thể thao'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/bong-da-p3','https://vnexpress.net/bong-da-p4',\n",
    "       'https://vnexpress.net/bong-da-p5','https://vnexpress.net/the-thao/tennis',\n",
    "      'https://vnexpress.net/the-thao/tennis-p2','https://vnexpress.net/the-thao/marathon',\n",
    "      'https://vnexpress.net/the-thao/marathon-p2','https://vnexpress.net/the-thao/cac-mon-khac/dua-xe',\n",
    "      'https://vnexpress.net/the-thao/cac-mon-khac/co-vua','https://vnexpress.net/the-thao/cac-mon-khac/golf']\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        sport.append([tag,content])\n",
    "    sport_df=pd.DataFrame(sport)\n",
    "\n",
    "    sport_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb01d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_news=[]\n",
    "tag = 'Thời sự'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/thoi-su','https://vnexpress.net/thoi-su-p2',\n",
    "      'https://vnexpress.net/thoi-su-p3','https://vnexpress.net/thoi-su-p4']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        daily_news.append([tag,content])\n",
    "    daily_news_df=pd.DataFrame(daily_news)\n",
    "\n",
    "    daily_news_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdc9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "worlds=[]\n",
    "tag = 'Thế giới'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/the-gioi','https://vnexpress.net/the-gioi-p2',\n",
    "      'https://vnexpress.net/the-gioi-p3','https://vnexpress.net/the-gioi-p4']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        worlds.append([tag,content])\n",
    "    worlds_df=pd.DataFrame(worlds)\n",
    "\n",
    "    worlds_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cfb6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecos=[]\n",
    "tag = 'Kinh tế'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/kinh-doanh/quoc-te','https://vnexpress.net/kinh-doanh/quoc-te-p2',\n",
    "      'https://vnexpress.net/kinh-doanh/quoc-te-p3','https://vnexpress.net/kinh-doanh/doanh-nghiep',\n",
    "      'https://vnexpress.net/kinh-doanh/doanh-nghiep-p2','https://vnexpress.net/kinh-doanh/chung-khoan',\n",
    "      'https://vnexpress.net/kinh-doanh/vi-mo','https://vnexpress.net/kinh-doanh/hang-hoa']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        ecos.append([tag,content])\n",
    "    ecos_df=pd.DataFrame(ecos)\n",
    "\n",
    "    ecos_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cddef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci=[]\n",
    "tag = 'Khoa học'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/khoa-hoc/khoa-hoc-trong-nuoc','https://vnexpress.net/khoa-hoc-p10',\n",
    "      'https://vnexpress.net/khoa-hoc/phat-minh','https://vnexpress.net/khoa-hoc/phat-minh-p2',\n",
    "      'https://vnexpress.net/khoa-hoc/ung-dung','https://vnexpress.net/khoa-hoc-p11',\n",
    "      'https://vnexpress.net/khoa-hoc-p12','https://vnexpress.net/khoa-hoc-p13']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        sci.append([tag,content])\n",
    "    sci_df=pd.DataFrame(sci)\n",
    "\n",
    "    sci_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4374a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent=[]\n",
    "tag = 'Giải trí'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/giai-tri-p10','https://vnexpress.net/giai-tri-p11',\n",
    "      'https://vnexpress.net/giai-tri-p12','https://vnexpress.net/giai-tri/gioi-sao',\n",
    "      'https://vnexpress.net/giai-tri/sach','https://vnexpress.net/giai-tri/phim',\n",
    "       'https://vnexpress.net/giai-tri/phim-p2','https://vnexpress.net/giai-tri/nhac',\n",
    "      'https://vnexpress.net/giai-tri/nhac-p2','https://vnexpress.net/giai-tri/thoi-trang']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        ent.append([tag,content])\n",
    "    ent_df=pd.DataFrame(ent)\n",
    "\n",
    "    ent_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea2136d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Thể thao</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0       Tag Content\n",
       "31         0.0  Thể thao     NaN"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('.\\data.csv')\n",
    "df[31:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26ae1d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_count=df['Content'].isna().sum()\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70847ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Content'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a5ca209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_count2=df['Content'].isnull().sum()\n",
    "missing_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdd139ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "532032fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thể thao</td>\n",
       "      <td>Mbappe không muốn rời PSG hè này vì tiếc 163 t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thể thao</td>\n",
       "      <td>Thanh Hóa bị cầm chân ở Nam ĐịnhSau khi thua C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thể thao</td>\n",
       "      <td>Quang Hải được biệt đãi thế nào ở CAHNNguyễn Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thể thao</td>\n",
       "      <td>Messi thừa nhận rạn nứt với fan PSGTrên tờ AS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thể thao</td>\n",
       "      <td>Mỹ đăng cai FIFA Club World Cup 2025Theo thông...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>Giải trí</td>\n",
       "      <td>Hôm 9/6, Nicola Peltz \"mặc như không\" khi đi ă...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6240</th>\n",
       "      <td>Giải trí</td>\n",
       "      <td>Chiều 15/6, trong buổi họp báo định kỳ, đại di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6241</th>\n",
       "      <td>Giải trí</td>\n",
       "      <td>Tại lễ trao giải Tony hôm 11/6, diễn viên Lupi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>Giải trí</td>\n",
       "      <td>Châu Diệu Minh là em dâu ca sĩ - diễn viên Min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>Giải trí</td>\n",
       "      <td>Đam mê thời trang, Thảo Nhi còn thành lập thươ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6139 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Tag                                            Content\n",
       "0     Thể thao  Mbappe không muốn rời PSG hè này vì tiếc 163 t...\n",
       "1     Thể thao  Thanh Hóa bị cầm chân ở Nam ĐịnhSau khi thua C...\n",
       "2     Thể thao  Quang Hải được biệt đãi thế nào ở CAHNNguyễn Q...\n",
       "3     Thể thao  Messi thừa nhận rạn nứt với fan PSGTrên tờ AS ...\n",
       "4     Thể thao  Mỹ đăng cai FIFA Club World Cup 2025Theo thông...\n",
       "...        ...                                                ...\n",
       "6239  Giải trí  Hôm 9/6, Nicola Peltz \"mặc như không\" khi đi ă...\n",
       "6240  Giải trí  Chiều 15/6, trong buổi họp báo định kỳ, đại di...\n",
       "6241  Giải trí  Tại lễ trao giải Tony hôm 11/6, diễn viên Lupi...\n",
       "6242  Giải trí  Châu Diệu Minh là em dâu ca sĩ - diễn viên Min...\n",
       "6243  Giải trí  Đam mê thời trang, Thảo Nhi còn thành lập thươ...\n",
       "\n",
       "[6139 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fff377b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.\\data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdcf43db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thể thao', '0', 'Thời sự', 'Thế giới', 'Kinh tế', 'Khoa học',\n",
       "       'Giải trí'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a51007c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2790</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4014</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5256</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5690</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tag Content\n",
       "30     0       1\n",
       "61     0       1\n",
       "122    0       1\n",
       "213    0       1\n",
       "334    0       1\n",
       "485    0       1\n",
       "686    0       1\n",
       "917    0       1\n",
       "1178   0       1\n",
       "1469   0       1\n",
       "1790   0       1\n",
       "1829   0       1\n",
       "1898   0       1\n",
       "1997   0       1\n",
       "2126   0       1\n",
       "2171   0       1\n",
       "2246   0       1\n",
       "2351   0       1\n",
       "2486   0       1\n",
       "2517   0       1\n",
       "2578   0       1\n",
       "2669   0       1\n",
       "2790   0       1\n",
       "2941   0       1\n",
       "3122   0       1\n",
       "3333   0       1\n",
       "3574   0       1\n",
       "3602   0       1\n",
       "3660   0       1\n",
       "3748   0       1\n",
       "3866   0       1\n",
       "4014   0       1\n",
       "4192   0       1\n",
       "4400   0       1\n",
       "4638   0       1\n",
       "4669   0       1\n",
       "4730   0       1\n",
       "4821   0       1\n",
       "4942   0       1\n",
       "5084   0       1\n",
       "5256   0       1\n",
       "5458   0       1\n",
       "5690   0       1\n",
       "5952   0       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Tag']=='0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73ae19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Tag']!='0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a33553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thể thao', 'Thời sự', 'Thế giới', 'Kinh tế', 'Khoa học',\n",
       "       'Giải trí'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tag'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
