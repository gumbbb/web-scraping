{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a293b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25-06-23'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "from datetime import date\n",
    "today= date.today();\n",
    "\n",
    "today = today.strftime(\"%d-%m-%y\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddae6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "sport=[]\n",
    "tag = 'Thể thao'\n",
    "# URL của trang web gốc\n",
    "url = 'https://vnexpress.net/bong-da'\n",
    "\n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "for news_title in news_titles:\n",
    "    title = news_title.text.strip()\n",
    "    link_element = news_title.find('a')\n",
    "    news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "    response = requests.get(news_link)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "    news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "    content_elements = news_soup.find_all('p', class_='Normal')\n",
    "    content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "    sport.append([tag,title + content])\n",
    "sport_df=pd.DataFrame(sport,columns=['Tag','Content'])\n",
    "\n",
    "sport_df.to_csv('.\\data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05921ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sport=[]\n",
    "tag = 'Thể thao'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/bong-da-p3','https://vnexpress.net/bong-da-p4',\n",
    "       'https://vnexpress.net/bong-da-p5','https://vnexpress.net/the-thao/tennis',\n",
    "      'https://vnexpress.net/the-thao/tennis-p2','https://vnexpress.net/the-thao/marathon',\n",
    "      'https://vnexpress.net/the-thao/marathon-p2','https://vnexpress.net/the-thao/cac-mon-khac/dua-xe',\n",
    "      'https://vnexpress.net/the-thao/cac-mon-khac/co-vua','https://vnexpress.net/the-thao/cac-mon-khac/golf']\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        sport.append([tag,content])\n",
    "    sport_df=pd.DataFrame(sport)\n",
    "\n",
    "    sport_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb01d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_news=[]\n",
    "tag = 'Thời sự'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/thoi-su','https://vnexpress.net/thoi-su-p2',\n",
    "      'https://vnexpress.net/thoi-su-p3','https://vnexpress.net/thoi-su-p4']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        daily_news.append([tag,content])\n",
    "    daily_news_df=pd.DataFrame(daily_news)\n",
    "\n",
    "    daily_news_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b409b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "worlds=[]\n",
    "tag = 'Thế giới'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/the-gioi','https://vnexpress.net/the-gioi-p2',\n",
    "      'https://vnexpress.net/the-gioi-p3','https://vnexpress.net/the-gioi-p4']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        worlds.append([tag,content])\n",
    "    worlds_df=pd.DataFrame(worlds)\n",
    "\n",
    "    worlds_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cfb6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecos=[]\n",
    "tag = 'Kinh tế'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/kinh-doanh/quoc-te','https://vnexpress.net/kinh-doanh/quoc-te-p2',\n",
    "      'https://vnexpress.net/kinh-doanh/quoc-te-p3','https://vnexpress.net/kinh-doanh/doanh-nghiep',\n",
    "      'https://vnexpress.net/kinh-doanh/doanh-nghiep-p2','https://vnexpress.net/kinh-doanh/chung-khoan',\n",
    "      'https://vnexpress.net/kinh-doanh/vi-mo','https://vnexpress.net/kinh-doanh/hang-hoa']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        ecos.append([tag,content])\n",
    "    ecos_df=pd.DataFrame(ecos)\n",
    "\n",
    "    ecos_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cddef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci=[]\n",
    "tag = 'Khoa học'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/khoa-hoc/khoa-hoc-trong-nuoc','https://vnexpress.net/khoa-hoc-p10',\n",
    "      'https://vnexpress.net/khoa-hoc/phat-minh','https://vnexpress.net/khoa-hoc/phat-minh-p2',\n",
    "      'https://vnexpress.net/khoa-hoc/ung-dung','https://vnexpress.net/khoa-hoc-p11',\n",
    "      'https://vnexpress.net/khoa-hoc-p12','https://vnexpress.net/khoa-hoc-p13']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        sci.append([tag,content])\n",
    "    sci_df=pd.DataFrame(sci)\n",
    "\n",
    "    sci_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f856abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent=[]\n",
    "tag = 'Giải trí'\n",
    "# URL của trang web gốc\n",
    "url = ['https://vnexpress.net/giai-tri-p10','https://vnexpress.net/giai-tri-p11',\n",
    "      'https://vnexpress.net/giai-tri-p12','https://vnexpress.net/giai-tri/gioi-sao',\n",
    "      'https://vnexpress.net/giai-tri/sach','https://vnexpress.net/giai-tri/phim',\n",
    "       'https://vnexpress.net/giai-tri/phim-p2','https://vnexpress.net/giai-tri/nhac',\n",
    "      'https://vnexpress.net/giai-tri/nhac-p2','https://vnexpress.net/giai-tri/thoi-trang']\n",
    "\n",
    "for urls in url:\n",
    "    \n",
    "# Gửi yêu cầu GET để lấy nội dung HTML của trang web gốc\n",
    "    response = requests.get(urls)\n",
    "    html_content = response.text\n",
    "\n",
    "# Tạo đối tượng BeautifulSoup để phân tích cú pháp HTML của trang web gốc\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tìm các phần tử có class \"title-news\" trong nội dung HTML của trang web gốc\n",
    "    news_titles = soup.find_all(class_='title-news')\n",
    "\n",
    "# Duyệt qua danh sách các tiêu đề tin tức và lấy nội dung từ thẻ <p> có class \"normall\"\n",
    "    for news_title in news_titles:\n",
    "        title = news_title.text.strip()\n",
    "        link_element = news_title.find('a')\n",
    "        news_link = link_element.get('href')\n",
    "    \n",
    "    # Gửi yêu cầu GET để lấy nội dung HTML của trang con\n",
    "        response = requests.get(news_link)\n",
    "        html_content = response.text\n",
    "    \n",
    "    # Tạo đối tượng BeautifulSoup mới để phân tích cú pháp HTML của trang con\n",
    "        news_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm nội dung từ thẻ <p> có class \"Normal\" trong trang con\n",
    "        content_elements = news_soup.find_all('p', class_='Normal')\n",
    "        content = ' '.join([element.text.strip() for element in content_elements])\n",
    "    \n",
    "        ent.append([tag,content])\n",
    "    ent_df=pd.DataFrame(ent)\n",
    "\n",
    "    ent_df.to_csv('.\\data.csv', encoding='utf-8',mode='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
